{"componentChunkName":"component---node-modules-gatsby-theme-buzzing-src-gatsby-theme-blog-core-templates-post-query-js","path":"/ja/reddit/r/stocks/comments/nzv3b1/tracking_the_value_of_3_reddit_scrapers_for_30/","result":{"data":{"site":{"siteMetadata":{"title":"国外股市热门","author":"Buzzing.cc","description":"用中文浏览国外股票社区里的热门讨论","keywords":["buzzing","美股","股票","股市"],"siteUrl":"https://stocks.buzzing.cc","telegram":"@stocks_top","iconUrl":"https://stocks.buzzing.cc/avatar.png","defaultSocialImageUrl":null,"social":[{"name":"Reddit Stocks","url":"https://www.reddit.com/r/stocks","external":true},{"name":"Reddit Investing","url":"https://www.reddit.com/r/investing","external":true},{"name":"Charlie Bilello's twitter","url":"https://twitter.com/charliebilello","external":true},{"name":"Buzzing","url":"https://www.buzzing.cc/","external":true}],"menuLinks":[{"name":"每周精选","url":"/issues","external":null}],"disqus":null,"utterances":null,"localize":[{"title":"Buzzing on Stocks","description":"See popular discussions in foreign stock communities in your native language","keywords":["buzzing","stocks","U.S. stocks"],"locale":"en","social":{"name":null,"url":null,"external":null},"menuLinks":[{"name":"Weekly Selection","url":"/en/issues","external":null}]},{"title":"國外股市熱門","description":"用中文瀏覽國外股票社區裡的熱門討論","keywords":["buzzing","美股","股票","股市"],"locale":"zh-Hant","social":null,"menuLinks":[{"name":"每週精選","url":"/zh-Hant/issues","external":null}]},{"title":"米国株式市場人気の話し合います","description":"人気の米国株式市場の話し合いまを日本語で閲覧","keywords":["buzzing","米国株式市場"],"locale":"ja","social":null,"menuLinks":[]}]}},"blogPost":{"id":"RedditPost-e3eb6f28-a678-578b-9011-e43890387643","excerpt":"Just like everyone else I want to find a way to use these Reddit scrapers to\nactually make some money. Since I'm a data nerd I track any that I can and\nanalyze their hit rate, profitability, etc. I was using Unbias very successfully\nand making good money until it stopped updating (see original post…","body":"<!-- SC_OFF --><div class=\"md\"><p>Just like everyone else I want to find a way to use these Reddit scrapers to actually make some money. Since I&#39;m a data nerd I track any that I can and analyze their hit rate, profitability, etc. I was using Unbias very successfully and making good money until it stopped updating (<a href=\"https://www.reddit.com/r/stocks/comments/m71xi8/a_month_of_tracking_stock_scrapers_for/\">see original post here</a>)</p>\n\n<p>&#x200B;</p>\n\n<p>Since then I have been tracking 3 scrapers and 1 methodology I saw here:</p>\n\n<ul>\n<li><a href=\"https://www.reddit.com/r/MillennialBets/wiki/index/user/theindulgery\">Millenial Bets</a></li>\n<li><a href=\"https://hype-rider.com/reddit\">Hype Rider</a></li>\n<li><a href=\"https://dayminer.herokuapp.com/\">Dayminer</a></li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1wBbawSwzpei5Q93Erg26Ii8JADFryoM2smpNTx5F9aM/edit#gid=910606400\">Using Finviz to see if buying the biggest losers of the day and selling the next morning for a 1% profit would work</a></li>\n</ul>\n\n<p>&#x200B;</p>\n\n<p><strong>Fair warning:</strong> Google Sheets struggles with large amounts of data and a lot of people logging in, so I created multiples of each spreadsheet. If you have trouble getting in all I can recommend is continuing to try. If you do, make a copy for yourself.</p>\n\n<p>&#x200B;</p>\n\n<p><strong>Summaries:</strong></p>\n\n<ul>\n<li><strong>Easiest to gather data: Millenial Bets</strong>. This is a straight copy/paste. The others required a lot of Excel copy, pasting, sorting, etc to get it to fit into the sheets</li>\n<li><strong>Best average returns: Millenial Bets.</strong> This may be because they had the biggest quantity of results. It&#39;s hard to narrow this down to just a few tickers to buy, so looks good as a paper trader but hard to put into use in the real world</li>\n<li><strong>Best at returning a small list of tickers to buy: Dayminer.</strong> This data narrows down to just a few, or sometimes none, per day. But the ones that do hit tend to hit pretty consistently</li>\n</ul>\n\n<p>&#x200B;</p>\n\n<p><strong>End Results from tracking the data:</strong> <em>(When paper trading I chose categories that had a % of Profitable vs Non-Profitable returns of greater than 90%. I was aiming for highest success rate of being profitable)</em></p>\n\n<ul>\n<li><p><strong>Millenial Bets:</strong> </p>\n\n<ul>\n<li><strong>Total data points:</strong> 5573</li>\n<li><strong>Number of paper trades:</strong> 2212</li>\n<li><strong>% return (avg):</strong> 11.44%</li>\n<li><strong>% return (max):</strong> 50%</li>\n<li><strong># of days to max profit (avg):</strong> 6<br/></li>\n</ul></li>\n<li><p><strong>Hype Rider:</strong> </p>\n\n<ul>\n<li><strong>Total data points:</strong> 5425</li>\n<li><strong>Number of paper trades:</strong> 714</li>\n<li><strong>% return (avg):</strong> 7.03%</li>\n<li><strong>% return (max): 13.19</strong>%</li>\n<li><strong># of days to max profit (avg):</strong> 4<br/></li>\n</ul></li>\n<li><p><strong>Dayminer:</strong> </p>\n\n<ul>\n<li><strong>Total data points:</strong> 5111</li>\n<li><strong>Number of paper trades:</strong> 278</li>\n<li><strong>% return (avg):</strong> 8.67%</li>\n<li><strong>% return (max):</strong> 29.77%</li>\n<li><strong># of days to max profit (avg):</strong> 5</li>\n</ul></li>\n</ul>\n\n<p>&#x200B;</p>\n\n<p><strong>How do I use this data?</strong> This is the question I get asked the most often. Here&#39;s what I do:</p>\n\n<ol>\n<li>Every morning before market open I copy the data from the scrapers, do whatever post-processing I need to do to get it to fit into the sheets, then paste into the sheets. This captures all the sentiment data from the last 24 hours</li>\n<li>When I&#39;m ready to buy something I look at the highest profitability categories in the data crunching tabs</li>\n<li>From there I narrow down THAT DAY&#39;S list of tickers based on the highest profitability categories</li>\n<li>I&#39;ll do a quick search on Reddit to see if overall sentiment is bullish or bearish. If everyone looks hopeful I&#39;ll buy</li>\n<li>I look at the average % returns and the average number of days to get there and those are my exit points<br/></li>\n</ol>\n\n<p>&#x200B;</p>\n\n<p>Now, on to the good stuff. Here are the links:<br/>\n<strong>Millenial Bets:</strong> <a href=\"https://imgur.com/gallery/nn14i7y\">Screen shots of the tables</a> | <a href=\"https://docs.google.com/spreadsheets/d/1iWKdaMUSdQ5G5ZUHabL7z5YKR-ZZalLqCD1Yv8ougZI/edit?usp=sharing\">Data (1)</a> | <a href=\"https://docs.google.com/spreadsheets/d/1wav7ZnQlk6NE1vMWFmx941fHzVMi_QFKlbtRRKsEaCY/edit?usp=sharing\">Data (2)</a> | <a href=\"https://docs.google.com/spreadsheets/d/1PXKCzJbva1NMAajASGNpmeO6LBVUT2PjZME3sS8S7yU/edit?usp=sharing\">Data (3)</a></p>\n\n<p><strong>Hype Rider:</strong> <a href=\"https://imgur.com/gallery/swbVZji\">Screen shots of the tables</a> | <a href=\"https://docs.google.com/spreadsheets/d/1vLUQTkZPzF924btqTWJXVrgmkLLHK4oOf1F1d3cSGP4/edit?usp=sharing\">Data (1)</a> | <a href=\"https://docs.google.com/spreadsheets/d/1GZUl3PeGSHMg8Hld4-9Z5COHWf_sjbI7q3AnwMZvwV4/edit?usp=sharing\">Data (2)</a> | <a href=\"https://docs.google.com/spreadsheets/d/1u5_nv1PxAdCvBN-m3gPODICrQA32UVz_FLEpskLm0Dw/edit?usp=sharing\">Data (3)</a></p>\n\n<p><strong>Dayminer</strong>: <a href=\"https://imgur.com/gallery/xUwlPTu\">Screen shots of tables</a> | <a href=\"https://docs.google.com/spreadsheets/d/1rRMFuvhHfC1SwZzZa1Y0AevB7U3dGmOECoKjUN9So8o/edit?usp=sharing\">Data (1)</a> | <a href=\"https://docs.google.com/spreadsheets/d/11-c3JLGOlpNx1A4Of_AE5Xq3nmoUSRJVCM-FmhXiSes/edit?usp=sharing\">Data (2)</a> | <a href=\"https://docs.google.com/spreadsheets/d/1ojmjOf0Xxzy0nppxCAhplw104f0btOzfGhbAHg5RRE4/edit?usp=sharing\">Data (3)</a></p>\n\n<p><strong>Finviz All Time Low data:</strong> <a href=\"https://imgur.com/gallery/UHnUUAU\">Screen shots of the tables</a> | <a href=\"https://docs.google.com/spreadsheets/d/1kQMqf6gMeRfaJgYUDPB2Mux7nXJX8KsH-J3bJaYF-Jw/edit?usp=sharing\">Data (1)</a>| <a href=\"https://docs.google.com/spreadsheets/d/1JnvE1ud8sdGstD28P8Y940icrajiifhOdxL0LyNVCQQ/edit?usp=sharing\">Data (2)</a> | <a href=\"https://docs.google.com/spreadsheets/d/1EdBUMWr36t0Ibq-Yjn-1OnXjLyeJ7-Tw6q0q_h043do/edit?usp=sharing\">Data (3)</a> (Nothing really clear came of this data so I didn&#39;t highlight it in the post)</p>\n</div><!-- SC_ON -->","slug":"/reddit/r/stocks/comments/nzv3b1/tracking_the_value_of_3_reddit_scrapers_for_30/","title":"Tracking the value of 3 Reddit scrapers for 30 days","tags":["stocks","reddit"],"date":"June 14, 2021","dateISO":"2021-06-14T21:23:20.000Z","datetime":"2021-06-14 21:23","image":null,"imageAlt":null,"socialImage":null,"__typename":"SocialMediaPost","thirdPartyId":"e3eb6f28-a678-578b-9011-e43890387643","provider":"Reddit","url":"https://www.reddit.com/r/stocks/comments/nzv3b1/tracking_the_value_of_3_reddit_scrapers_for_30/","originalUrl":"https://www.reddit.com/r/stocks/comments/nzv3b1/tracking_the_value_of_3_reddit_scrapers_for_30/","imageRemote":null,"video":null,"channel":"stocks","channelUrl":"https://www.reddit.com/r/stocks","author":"TheIndulgery","authorUrl":"https://www.reddit.com/user/TheIndulgery","authorImage":null,"authorImageRemote":null,"authorSlug":"TheIndulgery","score":202,"views":null,"sharedCount":null,"likeCount":null,"sharedContent":null,"parent":{"localize":[{"title":"3つのRedditスクレイパーの値を30日間追跡する","the_new_excerpt":"他の人と同じように、私もこのRedditスクレイパーを使って実際にお金を稼ぐ方法を見つけたいと思っています。\n実際にお金を稼ぐ方法を探しています。私はデータオタクなので、できる限りトラッキングして\nヒット率や収益性などを分析しています。私はUnbiasを非常にうまく使っていました。\n順調に稼いでいたのですが、更新が止まってしまいました（元記事参照）。","locale":"ja"},{"title":"追踪3个Reddit搜刮器的价值，为期30天","the_new_excerpt":"就像其他人一样，我想找到一种方法，利用这些Reddit刮刀来\n真正赚到一些钱。因为我是一个数据呆子，所以我追踪任何我可以追踪的东西，并\n分析他们的点击率、盈利能力等。我曾非常成功地使用Unbias\n赚了不少钱，直到它停止更新（见原帖...","locale":"zh"},{"title":"追蹤3個Reddit搜刮器的價值，爲期30天","the_new_excerpt":"就像其他人一樣，我想找到一種方法，利用這些Reddit刮刀來\n真正賺到一些錢。因爲我是一個數據呆子，所以我追蹤任何我可以追蹤的東西，並\n分析他們的點擊率、盈利能力等。我曾非常成功地使用Unbias\n賺了不少錢，直到它停止更新（見原帖...","locale":"zh-Hant"}]}},"previous":{"id":"RedditPost-38e87ad5-851c-5608-9e93-1299b5d98c05","excerpt":"What are your thoughts on ATOS? Going into the Russel 2000 on the 25th of June,\ntheir phase 2 trials of a breast cancer drug called Endoxifin were a success.\n\nI’m hoping they get bought out or partner with a bigger company, current share\nprice is $4.60 with a market cap of 557M.\n\nIs anyone else in…","slug":"/reddit/r/stocks/comments/nzogv1/thoughts_on_atos_undervalued/","title":"Thoughts on ATOS, undervalued?","date":"June 14, 2021","__typename":"SocialMediaPost","provider":"Reddit","parent":{"localize":[{"title":"ATOSの感想、過小評価？","the_new_excerpt":"ATOSについてのご意見をお聞かせください。6月25日に開催されるラッセル2000に参加します。\nエンドキシフィンと呼ばれる乳がん治療薬の第2相試験が成功しました。\n\n買収されるか、より大きな会社と提携することを期待しています。\n現在の株価は4.60ドル、時価総額は5億5700万円です。\n\n他に誰か...","locale":"ja"},{"title":"对ATOS的看法，价值被低估了吗？","the_new_excerpt":"你对ATOS有什么看法？6月25日将进入Russel 2000会议。\n他们对一种叫做Endoxifin的乳腺癌药物的第二阶段试验是成功的。\n\n我希望他们被收购或与更大的公司合作，目前的股份\n目前股价为4.6美元，市值为5.57亿美元。\n\n是否有人在...","locale":"zh"},{"title":"對ATOS的看法，價值被低估了嗎？","the_new_excerpt":"你對ATOS有什麼看法？6月25日將進入Russel 2000會議。\n他們對一種叫做Endoxifin的乳腺癌藥物的第二階段試驗是成功的。\n\n我希望他們被收購或與更大的公司合作，目前的股份\n目前股價爲4.6美元，市值爲5.57億美元。\n\n是否有人在...","locale":"zh-Hant"}]}},"next":{"id":"RedditPost-605a14dc-d95d-5c5f-91e1-6f444aa3fe17","excerpt":"These daily discussions run from Monday to Friday including during our themed\nposts.\n\nSome helpful links:\n\n * Finviz [https://finviz.com/quote.ashx?t=spy] for charts, fundamentals, and\n   aggregated news on individual stocks\n * Bloomberg market news [https://www.bloomberg.com/markets]\n *…","slug":"/reddit/r/stocks/comments/nziwu6/rstocks_daily_discussion_monday_jun_14_2021/","title":"r/Stocks Daily Discussion Monday - Jun 14, 2021","__typename":"SocialMediaPost","date":"June 14, 2021","provider":"Reddit","parent":{"localize":[{"title":"r/Stocks デイリーディスカッション 月曜日 - 2021年6月14日","the_new_excerpt":"毎日のディスカッションは、月曜から金曜まで、テーマごとに行われます。\nが行われます。\n\n参考になるリンクをご紹介します。\n\n * Finviz [https://finviz.com/quote.ashx?t=spy] チャート、ファンダメンタルズ、個別銘柄の集約されたニュースのための\n   チャート、ファンダメンタルズ、個別銘柄のニュースを見ることができます。\n * Bloomberg market news [https://www.bloomberg.com/markets]\n *...","locale":"ja"},{"title":"r/Stocks Daily Discussion Monday - Jun 14, 2021","the_new_excerpt":"这些日常讨论从周一到周五进行，包括在我们的主题性\n帖子。\n\n一些有用的链接。\n\n * Finviz [https://finviz.com/quote.ashx?t=spy] 用于图表、基本面、和\n   个别股票的汇总新闻\n * 彭博市场新闻[https://www.bloomberg.com/markets]\n *...","locale":"zh"},{"title":"r/Stocks Daily Discussion Monday - Jun 14, 2021","the_new_excerpt":"這些日常討論從週一到週五進行，包括在我們的主題性\n帖子。\n\n一些有用的鏈接。\n\n * Finviz [https://finviz.com/quote.ashx?t=spy] 用於圖表、基本面、和\n   個別股票的彙總新聞\n * 彭博市場新聞[https://www.bloomberg.com/markets]\n *...","locale":"zh-Hant"}]}}},"pageContext":{"basePath":"/","pageType":"detail","id":"RedditPost-e3eb6f28-a678-578b-9011-e43890387643","previousId":"RedditPost-38e87ad5-851c-5608-9e93-1299b5d98c05","nextId":"RedditPost-605a14dc-d95d-5c5f-91e1-6f444aa3fe17","maxWidth":1024,"siteMetadata":null,"locale":"ja","hrefLang":"ja-JA","originalPath":"/reddit/r/stocks/comments/nzv3b1/tracking_the_value_of_3_reddit_scrapers_for_30/","dateFormat":"YYYY-MM-DD"}},"staticQueryHashes":["1239077767","2744905544","3280999885"]}